{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclassification on the propensity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to create blocks of observations in the treatment and control group that have similar propensity scores. Then, take the difference in the outcome within those groups, and average the differences across groups to get an estimate of the ATE. In other words, we assume that there as a block-randomized experiment where treatment assignment was random within each block.\n",
    "\n",
    "How do we find the blocks? Imbens/Rubin (chapter 17.3) propose the following algorithm\n",
    "1. Start with a single block, with boundaries equal to 0 and 1\n",
    "2. Compute 2 statistics to decide whether the block should be further splitted or not:\n",
    "    - Compute t-statistic testing the H0 that the average value of the propensity score is the same in treatment and control groups. \n",
    "    - Compute the number of observations in treatment and control groups that would remain in two new blocks created by splitting the current block at the median of the propensity score \n",
    "3. While t-statistic > threshold and new block sizes sufficiently large:\n",
    "    - Split the block\n",
    "    - Update the statistics above \n",
    "\n",
    "\n",
    "We need to define the following parameters\n",
    "- t_max = 1.96\n",
    "- the minimum number of treated or control units in a new block\n",
    "- the minimum number of units (T + C) in a new block\n",
    "\n",
    "\n",
    "**NOTE** the code here is not working yet. need to load data as in the other notebook, and import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 1.96 \n",
    "N_min1 = 3 \n",
    "N_min2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load data here, following the other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsw_stacked.head()\n",
    "nsw_stacked.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use the linearized propensity score instead \n",
    "# nsw_stacked[\"lps\"] = np.log(nsw_stacked[\"pscore\"])\n",
    "data = nsw_stacked.loc[:, [\"treat\", \"lps\"]].to_numpy()\n",
    "b = np.ones((data.shape[0],1))\n",
    "data = np.c_[data, b]\n",
    "\n",
    "# compute the t-stat\n",
    "W = data[:, 0] #treatment indicator\n",
    "\n",
    "def summarise(a, block, W):\n",
    "    \"\"\"In a block for group W, compute:\n",
    "    - the average propensity score\n",
    "    - the number of observations\n",
    "    - the within-group variance\n",
    "    \"\"\"\n",
    "    # TODO: there are assumptions on which columns are which variable\n",
    "    # perhaps just use vectors directly?\n",
    "\n",
    "    N = np.sum(block * W)\n",
    "    l = 1 / N * np.transpose(a[:, 1] * block).dot(W)\n",
    "    sum_of_squared_diff = np.sum((a[:, 2] * block - l)**2)\n",
    "\n",
    "    return l, N, sum_of_squared_diff\n",
    "\n",
    "l_1, N_1, s2_1 = summarise(data, block=data[:, 2], W=W)\n",
    "l_0, N_0, s2_0 = summarise(data, block=data[:, 2], W=np.ones(W.shape) - W)\n",
    "s_hat = 1 / (N_1 + N_0 - 2) * (s2_0 + s2_1)\n",
    "\n",
    "t_stat = (l_1 - l_0) / (np.sqrt(s_hat * (1 / N_1 + 1 / N_0)))\n",
    "\n",
    "# count number of observation in a block if split by the median propensity score \n",
    "mid = np.median(data[:, 1])\n",
    "block_a = data[:, 1] <= mid \n",
    "block_b = data[:, 1] > mid \n",
    "\n",
    "np.sum(data[:, 2] * block_a * W )\n",
    "np.sum(data[:, 2] * block_a * (np.ones(W.shape) - W) )\n",
    "\n",
    "np.sum(data[:, 2] * block_b * W )\n",
    "# np.sum(data[:, 2] * block_b * (np.ones(W.shape) - W) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # matching without replacement -- working code\n",
    "# matched = []\n",
    "# for idx_treat in range(N_treat):\n",
    "#     distance = (treated[idx_treat, 1] - controls[:, 1])**2\n",
    "#     idx_min = np.argmin(distance) # index of the min \n",
    "#     idx_ctrl = controls[idx_min][0] # index of the matched control\n",
    "#     # update the pool of controls: drop the matched unit\n",
    "#     mask = np.full(distance.shape, True)\n",
    "#     mask[idx_min] = False\n",
    "#     controls = controls[mask, ...]\n",
    "#     # add the matched control index to matched_controls\n",
    "#     matched.append((int(idx_ctrl), idx_treat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
